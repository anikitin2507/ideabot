"""LLM client for generating decision advice (supports OpenRouter and OpenAI)."""

import asyncio
import httpx

import openai
import structlog
from openai import AsyncOpenAI

from src.config import Config

logger = structlog.get_logger()


class LLMClient:
    """Client for interacting with LLM APIs to generate decision advice."""

    def __init__(self, config: Config):
        """Initialize the LLM client."""
        self.config = config
        
        # Configure OpenAI client with appropriate base URL and API key
        self.client = AsyncOpenAI(
            api_key=config.api_key,
            base_url=config.api_base if config.api_type == "openrouter" else None
        )

    async def get_decision_advice(
        self,
        options: list[str],
        context: str | None = None,
        vote_results: dict[str, int] | None = None,
    ) -> str | None:
        """
        Generate decision advice for given options.

        Args:
            options: List of options to choose from
            context: Additional context from user (future feature)
            vote_results: Voting results from group chat (v1.1 feature)

        Returns:
            Decision advice string or None if failed
        """
        try:
            prompt = self._build_prompt(options, context, vote_results)

            logger.info(
                "Requesting decision advice from LLM",
                api_type=self.config.api_type,
                model=self.config.model,
                options_count=len(options),
            )

            # Build request parameters
            params = {
                "model": self.config.model,
                "messages": [
                    {"role": "system", "content": self._get_system_prompt()},
                    {"role": "user", "content": prompt},
                ],
                "max_tokens": 150,
                "temperature": 0.7,
                "timeout": self.config.response_timeout,
            }
            
            # Add OpenRouter specific headers if needed
            headers = {}
            if self.config.api_type == "openrouter":
                headers = {
                    "HTTP-Referer": "https://github.com/anikitin2507/ideabot",
                    "X-Title": "Decision Bot"
                }
                params["headers"] = headers

            response = await self.client.chat.completions.create(**params)

            if not response.choices:
                logger.error("No choices in LLM response")
                return None

            advice = response.choices[0].message.content
            if not advice:
                logger.error("Empty content in LLM response")
                return None

            advice = advice.strip()

            logger.info(
                "Generated decision advice successfully",
                advice_length=len(advice),
                tokens_used=response.usage.total_tokens if response.usage else None,
            )

            return advice

        except openai.RateLimitError as e:
            logger.error("LLM rate limit exceeded", error=str(e))
            return "üö´ –ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ AI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

        except openai.APIError as e:
            logger.error("LLM API error", error=str(e))
            return "üö´ –û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

        except httpx.TimeoutException:
            logger.error("LLM request timeout")
            return "‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ AI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

        except asyncio.TimeoutError:
            logger.error("LLM request timeout")
            return "‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ AI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

        except Exception as e:
            logger.error("Unexpected error in LLM client", error=str(e), error_type=type(e).__name__)
            return "üö´ –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    def _get_system_prompt(self) -> str:
        """Get the system prompt for the AI assistant."""
        return """–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤—ã–±—Ä–∞—Ç—å –æ–¥–∏–Ω –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –í—ã–±–µ—Ä–∏ –û–î–ò–ù –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö
2. –î–∞–π 1-2 –∫—Ä–∞—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º –≤—ã–±–æ—Ä–∞
3. –ë—É–¥—å –ª–∞–∫–æ–Ω–∏—á–µ–Ω –∏ –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω
4. –ò—Å–ø–æ–ª—å–∑—É–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: "–†–µ–∫–æ–º–µ–Ω–¥—É—é [–≤—ã–±—Ä–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç]. [–ö—Ä–∞—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ]."
"""

    def _build_prompt(
        self,
        options: list[str],
        context: str | None = None,
        vote_results: dict[str, int] | None = None,
    ) -> str:
        """Build the user prompt for decision making."""
        prompt_parts = []

        # Add options
        prompt_parts.append("–ü–æ–º–æ–≥–∏ –≤—ã–±—Ä–∞—Ç—å –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤:")
        for i, option in enumerate(options, 1):
            prompt_parts.append(f"{i}. {option}")

        # Add voting results if available (v1.1 feature)
        if vote_results:
            prompt_parts.append("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –¥—Ä—É–∑–µ–π:")
            total_votes = sum(vote_results.values())
            for option, votes in sorted(
                vote_results.items(), key=lambda x: x[1], reverse=True
            ):
                percentage = (votes / total_votes * 100) if total_votes > 0 else 0
                prompt_parts.append(f"‚Ä¢ {option}: {votes} –≥–æ–ª–æ—Å–æ–≤ ({percentage:.1f}%)")

        # Add context if provided
        if context:
            prompt_parts.append(f"\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {context}")

        return "\n".join(prompt_parts)


# For backward compatibility
OpenAIClient = LLMClient
